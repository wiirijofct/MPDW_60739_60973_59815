{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Week 1: ActivityNet Video Data Preparation and Indexing\n",
    "\n",
    "In this example we will use the ActivityNet dataset https://github.com/activitynet/ActivityNet. \n",
    "\n",
    " - Select the 10 videos with more moments.\n",
    " - Download these videos onto your computer.\n",
    " - Extract the frames for every video.\n",
    " - Read the textual descriptions of each video.\n",
    " - Index the video data in OpenSearch.\n",
    "\n",
    " In this week, you will index the video data and make it searchable with OpenSearch. You should refer to the OpenSearch tutorial laboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select videos\n",
    "Download the `activity_net.v1-3.min.json` file containing the list of videos. The file is in the github repository of ActivityNet.\n",
    "Parse this file and select the 10 videos with more moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import av\n",
    "import os\n",
    "import pprint as pp\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both JSON files\n",
    "with open('captions/val_1.json', 'r') as file1, open('captions/val_2.json', 'r') as file2:\n",
    "    data1 = json.load(file1)\n",
    "    data2 = json.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_timestamp_dictionaries(ident, dict1, dict2, res = {}):\n",
    "    # Extract all timestamp-string pairs from both dictionaries\n",
    "    pairs = []\n",
    "    dic_ident = \"v_\" + ident\n",
    "    # Add pairs from dict1\n",
    "    for i in range(len(dict1[dic_ident]['timestamps'])):\n",
    "        pairs.append((dict1[dic_ident]['timestamps'][i], dict1[dic_ident]['sentences'][i]))\n",
    "    \n",
    "    # Add pairs from dict2\n",
    "    for i in range(len(dict2[dic_ident]['timestamps'])):\n",
    "        pairs.append((dict2[dic_ident]['timestamps'][i], dict2[dic_ident]['sentences'][i]))\n",
    "    \n",
    "    # Sort pairs by timestamp\n",
    "    pairs.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Create new merged dictionary\n",
    "    res.update({ident: {\n",
    "        'duration': dict2[dic_ident][\"duration\"],\n",
    "        'timestamps': [pair[0] for pair in pairs],\n",
    "        'sentences': [pair[1] for pair in pairs]\n",
    "        }\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_videos_ids = [\"QKEFacWrn_8\", \"_15t4WTR19s\", \"eXMF6Skt2To\", \"TNFoUBRsngY\", \"od1jHUzgrAU\", \"gXk9TiqGUHs\", \"IEqnfSiCIXc\", \"Ez7s36AwgLk\", \"mHVmDOxtVt0\", \"i2X7z9ywHV8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in selected_videos_ids:\n",
    "    merge_timestamp_dictionaries(entry, data1, data2, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video frame extraction\n",
    "\n",
    "PyAV is a wrapper library providing you access to `ffmpeg`, a command-line video processing tool. In the example below, you will be able to extract frames from the a video shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"videos\"\n",
    "videos = [os.path.join(video_dir, vid) for vid in os.listdir(video_dir) if vid.endswith(\".mp4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['videos/IEqnfSiCIXc.mp4',\n",
       " 'videos/i2X7z9ywHV8.mp4',\n",
       " 'videos/TNFoUBRsngY.mp4',\n",
       " 'videos/Ez7s36AwgLk.mp4',\n",
       " 'videos/_15t4WTR19s.mp4',\n",
       " 'videos/mHVmDOxtVt0.mp4',\n",
       " 'videos/eXMF6Skt2To.mp4',\n",
       " 'videos/QKEFacWrn_8.mp4',\n",
       " 'videos/od1jHUzgrAU.mp4',\n",
       " 'videos/gXk9TiqGUHs.mp4']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = {}\n",
    "video_dir = \"videos\"\n",
    "frames_path = \"frames_dict.pkl\"\n",
    "\n",
    "if os.path.exists(frames_path):\n",
    "    with open(frames_path, 'rb') as f:\n",
    "        try:\n",
    "            frames = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Error loading pickle file:\", e)\n",
    "\n",
    "\n",
    "if not frames:\n",
    "    for vid in selected_videos_ids:\n",
    "        curr_dir = vid + \"_keyframes\"\n",
    "        frames_saved = os.path.isdir(curr_dir)\n",
    "        \n",
    "        if not frames_saved:\n",
    "            os.mkdir(curr_dir)\n",
    "    \n",
    "        \"\"\"\n",
    "        With this implementation we go through every frame and save them if they:\n",
    "         - are a key frame, or\n",
    "         - no other frame was saved within that second\n",
    "    \n",
    "        On top of that, we create the dictionary frames that contains every frame where:\n",
    "        - the key is: video_id + \"_\" + number_of_saved_frame\n",
    "        - the value is: another dictionary with \"timestamp\" and \"type\"\n",
    "        The type is either sec - for frames saved for the second they are in - or\n",
    "        key - for being a keyframe.\n",
    "        \"\"\"\n",
    "        with av.open(os.path.join(video_dir, vid + \".mp4\")) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            last_saved_second = -1\n",
    "    \n",
    "            i = 0\n",
    "            for j, frame in enumerate(container.decode(stream)):\n",
    "                if frame.pts is None:\n",
    "                    print(\"Something wrong here\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate the timestamp in seconds\n",
    "                timestamp = float(frame.pts * stream.time_base)\n",
    "                current_second = int(timestamp)\n",
    "                \n",
    "                # Check if this is a keyframe\n",
    "                is_keyframe = frame.key_frame\n",
    "                \n",
    "                # Determine if we should save this frame\n",
    "                save_frame = False\n",
    "                frame_type = None\n",
    "                \n",
    "                if is_keyframe:\n",
    "                    # Always save keyframes\n",
    "                    save_frame = True\n",
    "                    frame_type = \"key\"\n",
    "                elif current_second > last_saved_second:\n",
    "                    # Save non-keyframes only if we don't have a frame for this second yet\n",
    "                    save_frame = True\n",
    "                    frame_type = \"sec\"\n",
    "                \n",
    "                if save_frame:\n",
    "                    i+=1\n",
    "                    # Update the last second we saved a frame for\n",
    "                    last_saved_second = current_second\n",
    "                    \n",
    "                    # Create a descriptive frame name\n",
    "                    frame_name = f\"{vid}_{i}\"\n",
    "                    name = os.path.join(curr_dir, frame_name + \".jpg\")\n",
    "                    \n",
    "                    # Update the frames dictionary\n",
    "                    frames.update({\n",
    "                        frame_name: {\"timestamp\": timestamp,\n",
    "                                     \"type\": frame_type\n",
    "                                    }\n",
    "                    })\n",
    "                    \n",
    "                    # Save the frame image if required\n",
    "                    if not frames_saved:\n",
    "                        frame.to_image().save(name, quality=80)\n",
    "\n",
    "    with open(frames_path, 'wb') as f:\n",
    "        pickle.dump(frames, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video metadata\n",
    "\n",
    "Process the video metadata provided in the `json` file and index the video data in OpenSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the current OpenSearch Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = 'user08' # Add your user name here.\n",
    "password = '55LL.TTSS' # Add your user password here. For testing only. Don't store credentials in code. \n",
    "index_name = user\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index does not exist.\n"
     ]
    }
   ],
   "source": [
    "# host = 'localhost'\n",
    "# port = 9200\n",
    "\n",
    "# # Define the connection to the local OpenSearch server\n",
    "# client = OpenSearch(\n",
    "#     hosts = [{'host': host, 'port': port}],\n",
    "#     http_auth = ('admin', 'JIMMY\\neutron509'),  \n",
    "#     http_compress = True  # Enables gzip compression for request bodies\n",
    "#     #use_ssl = True,\n",
    "#     #verify_certs = False,\n",
    "#     #ssl_assert_hostname = False\n",
    "# )\n",
    "\n",
    "# index_name = \"wiirijo\"  # Replace with your actual index name\n",
    "\n",
    "# Check if index exists\n",
    "if client.indices.exists(index=index_name):\n",
    "    resp = client.indices.open(index=index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index=index_name)\n",
    "    pp.pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index=index_name)\n",
    "    pp.pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index=index_name))\n",
    "else:\n",
    "    print(\"Index does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Existing Index (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'user08' deleted.\n"
     ]
    }
   ],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])\n",
    "print(f\"Index '{index_name}' deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new index with mappings\n",
    "Play around here ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'user08' created.\n",
      "Index settings:\n",
      "{'user08': {'settings': {'index': {'creation_date': '1744731493704',\n",
      "                                   'knn': 'true',\n",
      "                                   'number_of_replicas': '0',\n",
      "                                   'number_of_shards': '4',\n",
      "                                   'provided_name': 'user08',\n",
      "                                   'replication': {'type': 'DOCUMENT'},\n",
      "                                   'uuid': 'XK9CcjL6TPuwZC-yAyL7fA',\n",
      "                                   'version': {'created': '136387927'}}}}}\n",
      "Index mappings:\n",
      "{'user08': {'mappings': {'dynamic': 'strict',\n",
      "                         'properties': {'caption': {'fields': {'keyword': {'type': 'keyword'}},\n",
      "                                                    'type': 'text'},\n",
      "                                        'caption_bow': {'type': 'text'},\n",
      "                                        'caption_id': {'type': 'keyword'},\n",
      "                                        'caption_vec': {'dimension': 768,\n",
      "                                                        'method': {'engine': 'nmslib',\n",
      "                                                                   'name': 'hnsw',\n",
      "                                                                   'parameters': {'ef_construction': 200,\n",
      "                                                                                  'm': 16},\n",
      "                                                                   'space_type': 'innerproduct'},\n",
      "                                                        'type': 'knn_vector'},\n",
      "                                        'duration': {'type': 'float'},\n",
      "                                        'end_timestamp': {'type': 'float'},\n",
      "                                        'keyframe_path': {'type': 'keyword'},\n",
      "                                        'keyframe_vec': {'dimension': 512,\n",
      "                                                         'method': {'engine': 'nmslib',\n",
      "                                                                    'name': 'hnsw',\n",
      "                                                                    'parameters': {'ef_construction': 200,\n",
      "                                                                                   'm': 16},\n",
      "                                                                    'space_type': 'innerproduct'},\n",
      "                                                         'type': 'knn_vector'},\n",
      "                                        'resolution': {'type': 'keyword'},\n",
      "                                        'start_timestamp': {'type': 'float'},\n",
      "                                        'video_id': {'type': 'keyword'}}}}}\n"
     ]
    }
   ],
   "source": [
    "index_body = {\n",
    "   \"settings\": {\n",
    "      \"index\": {\n",
    "         \"number_of_replicas\": 0,\n",
    "         \"number_of_shards\": 4,\n",
    "         # \"refresh_interval\": \"-1\", # Keep it off for now, change it to \"1s\" later (for searching)\n",
    "         \"knn\": \"true\"\n",
    "      }\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"dynamic\": \"strict\",\n",
    "      \"properties\": {\n",
    "         \"video_id\": {\"type\": \"keyword\"},\n",
    "         \"start_timestamp\": {\"type\": \"float\"},\n",
    "         \"end_timestamp\": {\"type\": \"float\"},\n",
    "         \"caption\": {\n",
    "            \"type\": \"text\",\n",
    "            \"fields\": {\n",
    "               \"keyword\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "         },\n",
    "         \"caption_id\": {\"type\": \"keyword\"},\n",
    "         \"caption_bow\": {\"type\": \"text\"},\n",
    "         \"caption_vec\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 768,\n",
    "            \"method\": {\n",
    "               \"name\": \"hnsw\",\n",
    "               \"space_type\": \"innerproduct\",\n",
    "               \"engine\": \"nmslib\",\n",
    "               \"parameters\": {\n",
    "                  \"m\": 16,\n",
    "                  \"ef_construction\": 200,\n",
    "               }\n",
    "            }\n",
    "         },\n",
    "         \"duration\": {\"type\": \"float\"},\n",
    "         \"resolution\": {\"type\": \"keyword\"},\n",
    "         \"keyframe_path\": {\"type\": \"keyword\"},\n",
    "         \"keyframe_vec\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 512,\n",
    "            \"method\": {\n",
    "               \"name\": \"hnsw\",\n",
    "               \"space_type\": \"innerproduct\",\n",
    "               \"engine\": \"nmslib\",\n",
    "               \"parameters\": {\n",
    "                  \"m\": 16,\n",
    "                  \"ef_construction\": 200,\n",
    "               }\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create the index\n",
    "response = client.indices.create(index=index_name, body=index_body)\n",
    "print(f\"Index '{index_name}' created.\")\n",
    "# Check the index settings\n",
    "settings = client.indices.get_settings(index=index_name)\n",
    "print(\"Index settings:\")\n",
    "pp.pprint(settings)\n",
    "# Check the index mappings\n",
    "mappings = client.indices.get_mapping(index=index_name)\n",
    "print(\"Index mappings:\")\n",
    "pp.pprint(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video captions\n",
    "\n",
    "The ActivityNetCaptions dataset https://cs.stanford.edu/people/ranjaykrishna/densevid/ dataset provides a textual description of each videos. Index the video captions on a text field of your OpenSearch index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating embeddings for Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiirijo/anaconda3/envs/mpdw/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model type: <class 'transformers.models.mpnet.modeling_mpnet.MPNetModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Loaded model type: {type(model)}\") \n",
    "\n",
    "def generate_caption_embedding(caption):\n",
    "    inputs = tokenizer(caption, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Pass the entire dictionary\n",
    "        # Mean pooling\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for Keyframes\n",
    "\n",
    "dependency:\n",
    "\n",
    "pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiirijo/anaconda3/envs/mpdw/lib/python3.9/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "def generate_keyframe_embedding(image_path):\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize\n",
    "        embedding_vector = image_features.cpu().numpy().flatten()\n",
    "        if len(embedding_vector) == 0:\n",
    "            print(f\"Failed to generate embedding for {image_path}\")\n",
    "            return None\n",
    "        return embedding_vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index refreshed.\n"
     ]
    }
   ],
   "source": [
    "# Get the activity dataset\n",
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    activity_data = json.load(json_data)\n",
    "# Specify the index name you want to check\n",
    "# index_name = \"wiirijo\"\n",
    "\n",
    "# Get document count for the index\n",
    "doc_count = client.count(index=index_name)\n",
    "\n",
    "if doc_count['count'] > 0:\n",
    "    print(\"Are you sure documents arent already indexed?\")\n",
    "else:\n",
    "    # Cycle through the videos\n",
    "    for video_id in selected_videos_ids:\n",
    "        video_info = db[video_id]\n",
    "        duration = video_info[\"duration\"]\n",
    "        timestamps = video_info[\"timestamps\"]\n",
    "        captions = video_info[\"sentences\"]\n",
    "        resolution = activity_data[\"database\"][video_id][\"resolution\"]\n",
    "        keyframe_dir = video_id + \"_keyframes\"\n",
    "        keyframes = [os.path.splitext(f)[0] for f in os.listdir(keyframe_dir)]\n",
    "    \n",
    "        # Cycle through the video keyframes\n",
    "        for frame_name in keyframes:\n",
    "    \n",
    "            kf_timestamp = frames[frame_name][\"timestamp\"]\n",
    "    \n",
    "            for i in range(len(timestamps)):\n",
    "    \n",
    "                start_time, end_time = timestamps[i]\n",
    "    \n",
    "                if start_time <= kf_timestamp <= end_time:\n",
    "                    \n",
    "                    caption = captions[i]\n",
    "                    caption_embedding = generate_caption_embedding(caption)\n",
    "                    caption_bow = caption.split()\n",
    "                    caption_vec = caption_embedding.tolist()\n",
    "                    keyframe_path = os.path.join(keyframe_dir, frame_name + \".jpg\")\n",
    "                    keyframe_embedding = generate_keyframe_embedding(keyframe_path)\n",
    "                    keyframe_vec = keyframe_embedding.tolist()\n",
    "                    \n",
    "                    caption_id = f\"{video_id}_{start_time}_{end_time}\"\n",
    "                    \n",
    "                    document_id = f\"{caption_id}_{frame_name}\"\n",
    "                    \n",
    "                    doc = {\n",
    "                            'video_id': video_id,\n",
    "                            'start_timestamp': start_time,\n",
    "                            'end_timestamp': end_time,\n",
    "                            'caption': caption,\n",
    "                            'caption_id': caption_id,\n",
    "                            'caption_bow': caption_bow,\n",
    "                            'caption_vec': caption_vec,\n",
    "                            'duration': duration,\n",
    "                            'resolution': resolution,\n",
    "                            'keyframe_path': keyframe_path,\n",
    "                            'keyframe_vec': keyframe_vec\n",
    "                        }\n",
    "                    \n",
    "                    try:\n",
    "                        response = client.index(index=index_name, id=document_id, body=doc)\n",
    "                        #print(f\"Indexed document for keyframe {frame_name} with caption '{caption}'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error indexing document for video {video_id}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    # Refresh the index to make the documents searchable\n",
    "    client.indices.refresh(index=index_name)\n",
    "    print(\"Index refreshed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in user08: 5967\n"
     ]
    }
   ],
   "source": [
    "# Specify the index name you want to check\n",
    "# index_name = \"wiirijo\"\n",
    "\n",
    "# Get document count for the index\n",
    "doc_count = client.count(index=index_name)\n",
    "\n",
    "# Print the document count\n",
    "print(f\"Number of documents in {index_name}: {doc_count['count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "Caption: A man is skating in a skate park., Score: 8.223228\n",
      "Caption: A man is skating in a skate park., Score: 8.223228\n",
      "Caption: A man is skating in a skate park., Score: 8.223228\n",
      "Caption: A man is skating in a skate park., Score: 8.223228\n",
      "Caption: A man is skating in a skate park., Score: 8.223228\n",
      "Caption: A man is skating in a skate park., Score: 8.223228\n",
      "Caption: A man is skating in a skate park., Score: 8.070235\n",
      "Caption: A man is skating in a skate park., Score: 8.070235\n",
      "Caption: A man is skating in a skate park., Score: 8.070235\n",
      "Caption: A man is skating in a skate park., Score: 8.070235\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"caption\": \"skateboarding in a park\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=query)\n",
    "print(\"Search Results:\")\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Caption: {hit['_source']['caption']}, Score: {hit['_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search with KNN Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiirijo/anaconda3/envs/mpdw/lib/python3.9/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Search Results:\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n",
      "Caption: A man is skating in a skate park., Score: 7.110288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Generate embedding for your query\n",
    "query_text = \"A man skateboarding on a park\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(query_text, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Construct the query for OpenSearch\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"caption_vec\": {\n",
    "                \"vector\": query_embedding.tolist(),\n",
    "                \"k\": 5  # Number of nearest neighbors to retrieve\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=query)\n",
    "print(\"KNN Search Results:\")\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Caption: {hit['_source']['caption']}, Score: {hit['_score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For distinct results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct KNN Search Results:\n",
      "Caption:  Men attempt various tricks using bowling balls., Count: 73\n",
      "Caption:  Other person skates on a park, then pass over the rails and turning and flipping teh skateboard., Count: 32\n",
      "Caption:   A man knocks down pins in a skating pool drop in., Count: 12\n",
      "Caption:  We see skating for awhile., Count: 8\n",
      "Caption:  After, a man interview a male using a microphone., Count: 6\n",
      "Caption:  Then, people talks in a bowling center, wile males trowing bowling bowls., Count: 6\n",
      "Caption: People are on front a board in a park holding bowling pins., Count: 5\n",
      "Caption: The video starts with footage of people outside with red,white and blue text explaining what the video is about., Count: 5\n",
      "Caption:   A bowling ball in a skate drop breaks something inside the drop., Count: 3\n",
      "Caption:   One man shows another how to hold the bowling ball., Count: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Generate embedding for your query\n",
    "query_text = \"skateboarding\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(query_text, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Construct the query for OpenSearch\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"caption_vec\": {\n",
    "                \"vector\": query_embedding.tolist(),\n",
    "                \"k\": 10  # Number of nearest neighbors to retrieve\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"distinct_captions\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"caption.keyword\",\n",
    "                \"size\": 10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=query)\n",
    "print(\"Distinct KNN Search Results:\")\n",
    "for bucket in response['aggregations']['distinct_captions']['buckets']:\n",
    "    print(f\"Caption: {bucket['key']}, Count: {bucket['doc_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Boolean Query Results:\n",
      "Caption: When he removes his hand from his throat,he takes another puff of the hookah and blows it out again., Count: 53\n",
      "Caption:  We see the man boxing with another man., Count: 46\n",
      "Caption:  The man uses the polish on his shoes., Count: 39\n",
      "Caption:  He then gets fully dressed, noticing his trembling feet., Count: 14\n"
     ]
    }
   ],
   "source": [
    "bool_query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"filter\": [\n",
    "                {\"term\": {\"resolution\": \"1920x1080\"}},\n",
    "                {\n",
    "                    \"range\": {\n",
    "                        \"start_timestamp\": {\n",
    "                            \"gte\": 60,\n",
    "                            \"lte\": 70\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"distinct_captions\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"caption.keyword\",\n",
    "                \"size\": 10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=bool_query)\n",
    "print(\"Distinct Boolean Query Results:\")\n",
    "for bucket in response['aggregations']['distinct_captions']['buckets']:\n",
    "    print(f\"Caption: {bucket['key']}, Count: {bucket['doc_count']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing embedding search + filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered KNN Search Results:\n",
      "Caption:  The man blows circles in the smoke., Count: 28\n",
      "Caption: We see a man smoking a hookah pipe and talking to the camera., Count: 12\n"
     ]
    }
   ],
   "source": [
    "query_text = \"smoking\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(query_text, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [  # Combine KNN search with other filters\n",
    "                {\n",
    "                    \"knn\": {\n",
    "                        \"caption_vec\": {\n",
    "                            \"vector\": query_embedding.tolist(),\n",
    "                            \"k\": 10  # Number of nearest neighbors to retrieve\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"filter\": [  # Apply filters before the KNN search\n",
    "                {\"term\": {\"resolution\": \"1920x1080\"}},\n",
    "                {\"range\": {\"duration\": {\"gte\": 150}}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"aggs\": {\n",
    "        \"distinct_captions\": {\n",
    "            \"terms\": {\n",
    "                \"field\": \"caption.keyword\",\n",
    "                \"size\": 10\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=query)\n",
    "print(\"Filtered KNN Search Results:\")\n",
    "for bucket in response['aggregations']['distinct_captions']['buckets']:\n",
    "    print(f\"Caption: {bucket['key']}, Count: {bucket['doc_count']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpdw",
   "language": "python",
   "name": "mpdw"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
