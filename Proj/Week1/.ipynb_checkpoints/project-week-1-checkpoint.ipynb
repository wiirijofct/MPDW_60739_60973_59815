{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Week 1: ActivityNet Video Data Preparation and Indexing\n",
    "\n",
    "In this example we will use the ActivityNet dataset https://github.com/activitynet/ActivityNet. \n",
    "\n",
    " - Select the 10 videos with more moments.\n",
    " - Download these videos onto your computer.\n",
    " - Extract the frames for every video.\n",
    " - Read the textual descriptions of each video.\n",
    " - Index the video data in OpenSearch.\n",
    "\n",
    " In this week, you will index the video data and make it searchable with OpenSearch. You should refer to the OpenSearch tutorial laboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select videos\n",
    "Download the `activity_net.v1-3.min.json` file containing the list of videos. The file is in the github repository of ActivityNet.\n",
    "Parse this file and select the 10 videos with more moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import av\n",
    "import os\n",
    "import pprint as pp\n",
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both JSON files\n",
    "with open('captions/val_1.json', 'r') as file1, open('captions/val_2.json', 'r') as file2:\n",
    "    data1 = json.load(file1)\n",
    "    data2 = json.load(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_timestamp_dictionaries(ident, dict1, dict2, res = {}):\n",
    "    # Extract all timestamp-string pairs from both dictionaries\n",
    "    pairs = []\n",
    "    dic_ident = \"v_\" + ident\n",
    "    # Add pairs from dict1\n",
    "    for i in range(len(dict1[dic_ident]['timestamps'])):\n",
    "        pairs.append((dict1[dic_ident]['timestamps'][i], dict1[dic_ident]['sentences'][i]))\n",
    "    \n",
    "    # Add pairs from dict2\n",
    "    for i in range(len(dict2[dic_ident]['timestamps'])):\n",
    "        pairs.append((dict2[dic_ident]['timestamps'][i], dict2[dic_ident]['sentences'][i]))\n",
    "    \n",
    "    # Sort pairs by timestamp\n",
    "    pairs.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Create new merged dictionary\n",
    "    res.update({ident: {\n",
    "        'duration': dict2[dic_ident][\"duration\"],\n",
    "        'timestamps': [pair[0] for pair in pairs],\n",
    "        'sentences': [pair[1] for pair in pairs]\n",
    "        }\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_videos_ids = [\"QKEFacWrn_8\", \"_15t4WTR19s\", \"eXMF6Skt2To\", \"TNFoUBRsngY\", \"od1jHUzgrAU\", \"gXk9TiqGUHs\", \"IEqnfSiCIXc\", \"Ez7s36AwgLk\", \"mHVmDOxtVt0\", \"i2X7z9ywHV8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in selected_videos_ids:\n",
    "    merge_timestamp_dictionaries(entry, data1, data2, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video frame extraction\n",
    "\n",
    "PyAV is a wrapper library providing you access to `ffmpeg`, a command-line video processing tool. In the example below, you will be able to extract frames from the a video shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"videos\"\n",
    "videos = [os.path.join(video_dir, vid) for vid in os.listdir(video_dir) if vid.endswith(\".mp4\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['videos/i2X7z9ywHV8.mp4',\n",
       " 'videos/mHVmDOxtVt0.mp4',\n",
       " 'videos/QKEFacWrn_8.mp4',\n",
       " 'videos/gXk9TiqGUHs.mp4',\n",
       " 'videos/IEqnfSiCIXc.mp4',\n",
       " 'videos/od1jHUzgrAU.mp4',\n",
       " 'videos/TNFoUBRsngY.mp4',\n",
       " 'videos/Ez7s36AwgLk.mp4',\n",
       " 'videos/dSdZz_Royyc.mp4',\n",
       " 'videos/eXMF6Skt2To.mp4',\n",
       " 'videos/_15t4WTR19s.mp4']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = {}\n",
    "video_dir = \"videos\"\n",
    "frames_path = \"frames_dict.pkl\"\n",
    "\n",
    "if os.path.exists(frames_path):\n",
    "    with open(frames_path, 'rb') as f:\n",
    "        try:\n",
    "            frames = pickle.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Error loading pickle file:\", e)\n",
    "\n",
    "\n",
    "if not frames:\n",
    "    for vid in selected_videos_ids:\n",
    "        curr_dir = vid + \"_keyframes\"\n",
    "        frames_saved = os.path.isdir(curr_dir)\n",
    "        \n",
    "        if not frames_saved:\n",
    "            os.mkdir(curr_dir)\n",
    "    \n",
    "        \"\"\"\n",
    "        With this implementation we go through every frame and save them if they:\n",
    "         - are a key frame, or\n",
    "         - no other frame was saved within that second\n",
    "    \n",
    "        On top of that, we create the dictionary frames that contains every frame where:\n",
    "        - the key is: video_id + \"_\" + number_of_saved_frame\n",
    "        - the value is: another dictionary with \"timestamp\" and \"type\"\n",
    "        The type is either sec - for frames saved for the second they are in - or\n",
    "        key - for being a keyframe.\n",
    "        \"\"\"\n",
    "        with av.open(os.path.join(video_dir, vid + \".mp4\")) as container:\n",
    "            stream = container.streams.video[0]\n",
    "            last_saved_second = -1\n",
    "    \n",
    "            i = 0\n",
    "            for j, frame in enumerate(container.decode(stream)):\n",
    "                if frame.pts is None:\n",
    "                    print(\"Something wrong here\")\n",
    "                    continue\n",
    "                \n",
    "                # Calculate the timestamp in seconds\n",
    "                timestamp = float(frame.pts * stream.time_base)\n",
    "                current_second = int(timestamp)\n",
    "                \n",
    "                # Check if this is a keyframe\n",
    "                is_keyframe = frame.key_frame\n",
    "                \n",
    "                # Determine if we should save this frame\n",
    "                save_frame = False\n",
    "                frame_type = None\n",
    "                \n",
    "                if is_keyframe:\n",
    "                    # Always save keyframes\n",
    "                    save_frame = True\n",
    "                    frame_type = \"key\"\n",
    "                elif current_second > last_saved_second:\n",
    "                    # Save non-keyframes only if we don't have a frame for this second yet\n",
    "                    save_frame = True\n",
    "                    frame_type = \"sec\"\n",
    "                \n",
    "                if save_frame:\n",
    "                    i+=1\n",
    "                    # Update the last second we saved a frame for\n",
    "                    last_saved_second = current_second\n",
    "                    \n",
    "                    # Create a descriptive frame name\n",
    "                    frame_name = f\"{vid}_{i}\"\n",
    "                    name = os.path.join(curr_dir, frame_name + \".jpg\")\n",
    "                    \n",
    "                    # Update the frames dictionary\n",
    "                    frames.update({\n",
    "                        frame_name: {\"timestamp\": timestamp,\n",
    "                                     \"type\": frame_type\n",
    "                                    }\n",
    "                    })\n",
    "                    \n",
    "                    # Save the frame image if required\n",
    "                    if not frames_saved:\n",
    "                        frame.to_image().save(name, quality=80)\n",
    "\n",
    "    with open(frames_path, 'wb') as f:\n",
    "        pickle.dump(frames, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video metadata\n",
    "\n",
    "Process the video metadata provided in the `json` file and index the video data in OpenSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the current OpenSearch Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX SETTINGS\n",
      "{'wiirijo': {'settings': {'index': {'creation_date': '1744713255323',\n",
      "                                    'knn': 'true',\n",
      "                                    'number_of_replicas': '0',\n",
      "                                    'number_of_shards': '4',\n",
      "                                    'provided_name': 'wiirijo',\n",
      "                                    'refresh_interval': '-1',\n",
      "                                    'replication': {'type': 'DOCUMENT'},\n",
      "                                    'uuid': 'AeBZuNqaRsq82Gvj_YKS_g',\n",
      "                                    'version': {'created': '136407927'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX MAPPINGS\n",
      "{'wiirijo': {'mappings': {'dynamic': 'strict',\n",
      "                          'properties': {'caption': {'type': 'text'},\n",
      "                                         'caption_bow': {'type': 'text'},\n",
      "                                         'caption_vec': {'dimension': 768,\n",
      "                                                         'method': {'engine': 'nmslib',\n",
      "                                                                    'name': 'hnsw',\n",
      "                                                                    'parameters': {'ef_construction': 200,\n",
      "                                                                                   'm': 16},\n",
      "                                                                    'space_type': 'innerproduct'},\n",
      "                                                         'type': 'knn_vector'},\n",
      "                                         'duration': {'type': 'float'},\n",
      "                                         'end_timestamp': {'type': 'float'},\n",
      "                                         'keyframe_path': {'type': 'keyword'},\n",
      "                                         'keyframe_vec': {'dimension': 512,\n",
      "                                                          'method': {'engine': 'nmslib',\n",
      "                                                                     'name': 'hnsw',\n",
      "                                                                     'parameters': {'ef_construction': 200,\n",
      "                                                                                    'm': 16},\n",
      "                                                                     'space_type': 'innerproduct'},\n",
      "                                                          'type': 'knn_vector'},\n",
      "                                         'resolution': {'type': 'keyword'},\n",
      "                                         'start_timestamp': {'type': 'float'},\n",
      "                                         'video_id': {'type': 'keyword'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX #DOCs\n",
      "{'count': 0, '_shards': {'total': 4, 'successful': 4, 'skipped': 0, 'failed': 0}}\n"
     ]
    }
   ],
   "source": [
    "host = 'localhost'\n",
    "port = 9200\n",
    "\n",
    "# Define the connection to the local OpenSearch server\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_auth = ('admin', 'JIMMY\\neutron509'),  \n",
    "    http_compress = True  # Enables gzip compression for request bodies\n",
    "    #use_ssl = True,\n",
    "    #verify_certs = False,\n",
    "    #ssl_assert_hostname = False\n",
    ")\n",
    "\n",
    "index_name = \"wiirijo\"  # Replace with your actual index name\n",
    "\n",
    "# Check if index exists\n",
    "if client.indices.exists(index=index_name):\n",
    "    resp = client.indices.open(index=index_name)\n",
    "    print(resp)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX SETTINGS')\n",
    "    settings = client.indices.get_settings(index=index_name)\n",
    "    pp.pprint(settings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX MAPPINGS')\n",
    "    mappings = client.indices.get_mapping(index=index_name)\n",
    "    pp.pprint(mappings)\n",
    "\n",
    "    print('\\n----------------------------------------------------------------------------------- INDEX #DOCs')\n",
    "    print(client.count(index=index_name))\n",
    "else:\n",
    "    print(\"Index does not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Existing Index (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'wiirijo' deleted.\n"
     ]
    }
   ],
   "source": [
    "client.indices.delete(index=index_name, ignore=[400, 404])\n",
    "print(f\"Index '{index_name}' deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new index with mappings\n",
    "Play around here ig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'wiirijo' created.\n",
      "Index settings:\n",
      "{'wiirijo': {'settings': {'index': {'creation_date': '1744713255323',\n",
      "                                    'knn': 'true',\n",
      "                                    'number_of_replicas': '0',\n",
      "                                    'number_of_shards': '4',\n",
      "                                    'provided_name': 'wiirijo',\n",
      "                                    'refresh_interval': '-1',\n",
      "                                    'replication': {'type': 'DOCUMENT'},\n",
      "                                    'uuid': 'AeBZuNqaRsq82Gvj_YKS_g',\n",
      "                                    'version': {'created': '136407927'}}}}}\n",
      "Index mappings:\n",
      "{'wiirijo': {'mappings': {'dynamic': 'strict',\n",
      "                          'properties': {'caption': {'type': 'text'},\n",
      "                                         'caption_bow': {'type': 'text'},\n",
      "                                         'caption_vec': {'dimension': 768,\n",
      "                                                         'method': {'engine': 'nmslib',\n",
      "                                                                    'name': 'hnsw',\n",
      "                                                                    'parameters': {'ef_construction': 200,\n",
      "                                                                                   'm': 16},\n",
      "                                                                    'space_type': 'innerproduct'},\n",
      "                                                         'type': 'knn_vector'},\n",
      "                                         'duration': {'type': 'float'},\n",
      "                                         'end_timestamp': {'type': 'float'},\n",
      "                                         'keyframe_path': {'type': 'keyword'},\n",
      "                                         'keyframe_vec': {'dimension': 512,\n",
      "                                                          'method': {'engine': 'nmslib',\n",
      "                                                                     'name': 'hnsw',\n",
      "                                                                     'parameters': {'ef_construction': 200,\n",
      "                                                                                    'm': 16},\n",
      "                                                                     'space_type': 'innerproduct'},\n",
      "                                                          'type': 'knn_vector'},\n",
      "                                         'resolution': {'type': 'keyword'},\n",
      "                                         'start_timestamp': {'type': 'float'},\n",
      "                                         'video_id': {'type': 'keyword'}}}}}\n"
     ]
    }
   ],
   "source": [
    "index_body = {\n",
    "   \"settings\": {\n",
    "      \"index\": {\n",
    "         \"number_of_replicas\": 0,\n",
    "         \"number_of_shards\": 4,\n",
    "         \"refresh_interval\": \"-1\", # Keep it off for now, change it to \"1s\" later (for searching)\n",
    "         \"knn\": \"true\"\n",
    "      }\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"dynamic\": \"strict\",\n",
    "      \"properties\": {\n",
    "         \"video_id\": {\"type\": \"keyword\"},\n",
    "         \"start_timestamp\": {\"type\": \"float\"},\n",
    "         \"end_timestamp\": {\"type\": \"float\"},\n",
    "         \"caption\": {\"type\": \"text\"},\n",
    "         \"caption_bow\": {\"type\": \"text\"},\n",
    "         \"caption_vec\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 768,\n",
    "            \"method\": {\n",
    "               \"name\": \"hnsw\",\n",
    "               \"space_type\": \"innerproduct\",\n",
    "               \"engine\": \"nmslib\",\n",
    "               \"parameters\": {\n",
    "                  \"m\": 16,\n",
    "                  \"ef_construction\": 200,\n",
    "               }\n",
    "            }\n",
    "         },\n",
    "         \"duration\": {\"type\": \"float\"},\n",
    "         \"resolution\": {\"type\": \"keyword\"},\n",
    "         \"keyframe_path\": {\"type\": \"keyword\"},\n",
    "         \"keyframe_vec\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 512,\n",
    "            \"method\": {\n",
    "               \"name\": \"hnsw\",\n",
    "               \"space_type\": \"innerproduct\",\n",
    "               \"engine\": \"nmslib\",\n",
    "               \"parameters\": {\n",
    "                  \"m\": 16,\n",
    "                  \"ef_construction\": 200,\n",
    "               }\n",
    "            }\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create the index\n",
    "response = client.indices.create(index=index_name, body=index_body)\n",
    "print(f\"Index '{index_name}' created.\")\n",
    "# Check the index settings\n",
    "settings = client.indices.get_settings(index=index_name)\n",
    "print(\"Index settings:\")\n",
    "pp.pprint(settings)\n",
    "# Check the index mappings\n",
    "mappings = client.indices.get_mapping(index=index_name)\n",
    "print(\"Index mappings:\")\n",
    "pp.pprint(mappings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video captions\n",
    "\n",
    "The ActivityNetCaptions dataset https://cs.stanford.edu/people/ranjaykrishna/densevid/ dataset provides a textual description of each videos. Index the video captions on a text field of your OpenSearch index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating embeddings for Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model type: <class 'transformers.models.mpnet.modeling_mpnet.MPNetModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(f\"Loaded model type: {type(model)}\") \n",
    "\n",
    "def generate_caption_embedding(caption):\n",
    "    inputs = tokenizer(caption, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)  # Pass the entire dictionary\n",
    "        # Mean pooling\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings for Keyframes\n",
    "\n",
    "dependency:\n",
    "\n",
    "pip install openai-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gigag/Documents/mpdw/1env/lib/python3.12/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import open_clip\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n",
    "clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "def generate_keyframe_embedding(image_path):\n",
    "    try:\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize\n",
    "        embedding_vector = image_features.cpu().numpy().flatten()\n",
    "        if len(embedding_vector) == 0:\n",
    "            print(f\"Failed to generate embedding for {image_path}\")\n",
    "            return None\n",
    "        return embedding_vector\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m video_file \u001b[38;5;129;01min\u001b[39;00m video_files:\n\u001b[32m     46\u001b[39m     video_id = video_file.split(\u001b[33m'\u001b[39m\u001b[33m[\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mselected_videos\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m]\u001b[49m = \u001b[33m'\u001b[39m\u001b[33mv_\u001b[39m\u001b[33m'\u001b[39m + video_id\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m clean_id, caption_ds_id \u001b[38;5;129;01min\u001b[39;00m selected_videos.items():\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m caption_ds_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m combined_data:\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "from opensearchpy import helpers\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "host = 'api.novasearch.org'\n",
    "port = 443\n",
    "\n",
    "user = 'user08' # Add your user name here.\n",
    "password = '55LL.TTSS' # Add your user password here. For testing only. Don't store credentials in code. \n",
    "index_name = user\n",
    "\n",
    "# Create the client with SSL/TLS enabled, but hostname verification disabled.\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_compress = True, # enables gzip compression for request bodies\n",
    "    http_auth = (user, password),\n",
    "    use_ssl = True,\n",
    "    url_prefix = 'opensearch_v2',\n",
    "    verify_certs = False,\n",
    "    ssl_assert_hostname = False,\n",
    "    ssl_show_warn = False\n",
    ")\n",
    "\n",
    "# Load combined JSON data\n",
    "with open('captions/val_1.json', 'r') as file1, open('captions/val_2.json', 'r') as file2:\n",
    "    val1_data = json.load(file1)\n",
    "    val2_data = json.load(file2)\n",
    "combined_data = {**val1_data, **val2_data}\n",
    "\n",
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    activity_data = json.load(json_data)\n",
    "\n",
    "# Output directory for frames\n",
    "video_dir = 'videos'\n",
    "output_dir = 'keyframes'\n",
    "\n",
    "# List all video files in the directory (already downloaded)\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.mp4')]\n",
    "\n",
    "# extract video ID from the filename\n",
    "selected_videos = []\n",
    "for video_file in video_files:\n",
    "    video_id = video_file.split('[')[-1].split(']')[0]\n",
    "    selected_videos[video_id] = 'v_' + video_id\n",
    "\n",
    "\n",
    "for clean_id, caption_ds_id in selected_videos.items():\n",
    "    if caption_ds_id not in combined_data:\n",
    "        print(f\"Video ID {video_id} not found in either val_1 or val_2 datasets.\")\n",
    "        continue\n",
    "    \n",
    "    video_data = combined_data[caption_ds_id]\n",
    "    duration = video_data['duration']\n",
    "    timestamps = video_data['timestamps']\n",
    "    captions = video_data['sentences']\n",
    "    \n",
    "    # fetch resolution from activity_net.v1-3.min.json\n",
    "    if clean_id in activity_data['database']:\n",
    "        video_metadata = activity_data['database'][clean_id]\n",
    "        resolution = video_metadata.get('resolution', 'unknown')\n",
    "    else:\n",
    "        resolution = 'unknown'\n",
    "        \n",
    "        \n",
    "    for idx, (timestamp, caption) in enumerate(zip(timestamps, captions)):\n",
    "        start_time, end_time = timestamp\n",
    "        caption_embedding = generate_caption_embedding(caption)\n",
    "        caption_bow = ' '.join(caption.split())\n",
    "        caption_vec = caption_embedding.tolist()\n",
    "        \n",
    "        # keyframe image extraction\n",
    "        keyframe_path_val1 = os.path.join(output_dir, f\"{video_id}_val_1_frame_{idx}.jpg\")\n",
    "        keyframe_path_val2 = os.path.join(output_dir, f\"{video_id}_val_2_frame_{idx}.jpg\")\n",
    "        keyframe_path = keyframe_path_val1 if os.path.exists(keyframe_path_val1) else keyframe_path_val2\n",
    "        if not os.path.exists(keyframe_path):\n",
    "            print(f\"Keyframe image not found for {video_id} at index {idx}.\")\n",
    "            continue\n",
    "\n",
    "        keyframe_embedding = generate_keyframe_embedding(keyframe_path)\n",
    "        keyframe_vec = keyframe_embedding.tolist()\n",
    "        \n",
    "        # Prepare the document to be indexed\n",
    "        doc = {\n",
    "            'video_id': video_id,\n",
    "            'start_timestamp': start_time,\n",
    "            'end_timestamp': end_time,\n",
    "            'caption': caption,\n",
    "            'caption_bow': caption_bow,\n",
    "            'caption_vec': caption_vec,\n",
    "            'duration': duration,\n",
    "            'resolution': resolution,\n",
    "            'keyframe_path': keyframe_path,\n",
    "            'keyframe_vec': keyframe_vec\n",
    "        }\n",
    "        \n",
    "        # Index the document\n",
    "        try:\n",
    "            response = client.index(index=index_name, body=doc)\n",
    "            print(f\"Indexed document for video {video_id} with caption '{caption}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document for video {video_id}: {e}\")\n",
    "            continue\n",
    "# Refresh the index to make the documents searchable\n",
    "client.indices.refresh(index=index_name)\n",
    "print(\"Index refreshed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you sure documents arent already indexed?\n"
     ]
    }
   ],
   "source": [
    "# Get the activity dataset\n",
    "with open('activity_net.v1-3.min.json', 'r') as json_data:\n",
    "    activity_data = json.load(json_data)\n",
    "# Specify the index name you want to check\n",
    "index_name = \"wiirijo\"\n",
    "\n",
    "# Get document count for the index\n",
    "doc_count = client.count(index=index_name)\n",
    "\n",
    "if doc_count['count'] > 0:\n",
    "    print(\"Are you sure documents arent already indexed?\")\n",
    "else:\n",
    "    # Cycle through the videos\n",
    "    for video_id in selected_videos_ids:\n",
    "        video_info = db[video_id]\n",
    "        duration = video_info[\"duration\"]\n",
    "        timestamps = video_info[\"timestamps\"]\n",
    "        captions = video_info[\"sentences\"]\n",
    "        resolution = activity_data[\"database\"][video_id][\"resolution\"]\n",
    "        keyframe_dir = video_id + \"_keyframes\"\n",
    "        keyframes = [os.path.splitext(f)[0] for f in os.listdir(keyframe_dir)]\n",
    "    \n",
    "        # Cycle through the video keyframes\n",
    "        for frame_name in keyframes:\n",
    "    \n",
    "            kf_timestamp = frames[frame_name][\"timestamp\"]\n",
    "    \n",
    "            for i in range(len(timestamps)):\n",
    "    \n",
    "                start_time, end_time = timestamps[i]\n",
    "    \n",
    "                if start_time <= kf_timestamp <= end_time:\n",
    "                    \n",
    "                    caption = captions[i]\n",
    "                    caption_embedding = generate_caption_embedding(caption)\n",
    "                    caption_bow = ' '.join(caption.split())\n",
    "                    caption_vec = caption_embedding.tolist()\n",
    "                    keyframe_path = os.path.join(keyframe_dir, frame_name + \".jpg\")\n",
    "                    keyframe_embedding = generate_keyframe_embedding(keyframe_path)\n",
    "                    keyframe_vec = keyframe_embedding.tolist()\n",
    "                    \n",
    "                    doc = {\n",
    "                            'video_id': video_id,\n",
    "                            'start_timestamp': start_time,\n",
    "                            'end_timestamp': end_time,\n",
    "                            'caption': caption,\n",
    "                            'caption_bow': caption_bow,\n",
    "                            'caption_vec': caption_vec,\n",
    "                            'duration': duration,\n",
    "                            'resolution': resolution,\n",
    "                            'keyframe_path': keyframe_path,\n",
    "                            'keyframe_vec': keyframe_vec\n",
    "                        }\n",
    "                    \n",
    "                    try:\n",
    "                        response = client.index(index=index_name, body=doc)\n",
    "                        #print(f\"Indexed document for keyframe {frame_name} with caption '{caption}'\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error indexing document for video {video_id}: {e}\")\n",
    "                        continue\n",
    "\n",
    "    # Refresh the index to make the documents searchable\n",
    "    client.indices.refresh(index=index_name)\n",
    "    print(\"Index refreshed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "caption = \"A person is skateboarding in a park.\"\n",
    "embedding = generate_caption_embedding(caption)\n",
    "print(embedding.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wiirijo': {'mappings': {'dynamic': 'strict', 'properties': {'caption': {'type': 'text'}, 'caption_bow': {'type': 'text'}, 'caption_vec': {'type': 'knn_vector', 'dimension': 768, 'method': {'engine': 'nmslib', 'space_type': 'innerproduct', 'name': 'hnsw', 'parameters': {'ef_construction': 200, 'm': 16}}}, 'duration': {'type': 'float'}, 'end_timestamp': {'type': 'float'}, 'keyframe_path': {'type': 'keyword'}, 'keyframe_vec': {'type': 'knn_vector', 'dimension': 512, 'method': {'engine': 'nmslib', 'space_type': 'innerproduct', 'name': 'hnsw', 'parameters': {'ef_construction': 200, 'm': 16}}}, 'resolution': {'type': 'keyword'}, 'start_timestamp': {'type': 'float'}, 'video_id': {'type': 'keyword'}}}}}\n"
     ]
    }
   ],
   "source": [
    "mapping = client.indices.get_mapping(index=index_name)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Index Health and Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Health:\n",
      "{'active_primary_shards': 11,\n",
      " 'active_shards': 11,\n",
      " 'active_shards_percent_as_number': 78.57142857142857,\n",
      " 'cluster_name': 'docker-cluster',\n",
      " 'delayed_unassigned_shards': 0,\n",
      " 'discovered_cluster_manager': True,\n",
      " 'discovered_master': True,\n",
      " 'initializing_shards': 0,\n",
      " 'number_of_data_nodes': 1,\n",
      " 'number_of_in_flight_fetch': 0,\n",
      " 'number_of_nodes': 1,\n",
      " 'number_of_pending_tasks': 0,\n",
      " 'relocating_shards': 0,\n",
      " 'status': 'yellow',\n",
      " 'task_max_waiting_in_queue_millis': 0,\n",
      " 'timed_out': False,\n",
      " 'unassigned_shards': 3}\n",
      "\n",
      "Index Mapping:\n",
      "{'wiirijo': {'mappings': {'dynamic': 'strict',\n",
      "                          'properties': {'caption': {'type': 'text'},\n",
      "                                         'caption_bow': {'type': 'text'},\n",
      "                                         'caption_vec': {'dimension': 768,\n",
      "                                                         'method': {'engine': 'nmslib',\n",
      "                                                                    'name': 'hnsw',\n",
      "                                                                    'parameters': {'ef_construction': 200,\n",
      "                                                                                   'm': 16},\n",
      "                                                                    'space_type': 'innerproduct'},\n",
      "                                                         'type': 'knn_vector'},\n",
      "                                         'duration': {'type': 'float'},\n",
      "                                         'end_timestamp': {'type': 'float'},\n",
      "                                         'keyframe_path': {'type': 'keyword'},\n",
      "                                         'keyframe_vec': {'dimension': 512,\n",
      "                                                          'method': {'engine': 'nmslib',\n",
      "                                                                     'name': 'hnsw',\n",
      "                                                                     'parameters': {'ef_construction': 200,\n",
      "                                                                                    'm': 16},\n",
      "                                                                     'space_type': 'innerproduct'},\n",
      "                                                          'type': 'knn_vector'},\n",
      "                                         'resolution': {'type': 'keyword'},\n",
      "                                         'start_timestamp': {'type': 'float'},\n",
      "                                         'video_id': {'type': 'keyword'}}}}}\n"
     ]
    }
   ],
   "source": [
    "# Check the health of the OpenSearch cluster\n",
    "cluster_health = client.cluster.health()\n",
    "print(\"Cluster Health:\")\n",
    "pp.pprint(cluster_health)\n",
    "\n",
    "# Retrieve and print the mapping of the index\n",
    "index_mapping = client.indices.get_mapping(index=index_name)\n",
    "print(\"\\nIndex Mapping:\")\n",
    "pp.pprint(index_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in wiirijo: 6030\n"
     ]
    }
   ],
   "source": [
    "# Specify the index name you want to check\n",
    "index_name = \"wiirijo\"\n",
    "\n",
    "# Get document count for the index\n",
    "doc_count = client.count(index=index_name)\n",
    "\n",
    "# Print the document count\n",
    "print(f\"Number of documents in {index_name}: {doc_count['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Results:\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n",
      "Caption: A man is skating in a skate park., Score: 7.855377\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"caption\": \"skateboarding in a park\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=query)\n",
    "print(\"Search Results:\")\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Caption: {hit['_source']['caption']}, Score: {hit['_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search with KNN Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Search Results:\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n",
      "Caption: A man is skating in a skate park., Score: 7.1102877\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Generate embedding for your query\n",
    "query_text = \"A man skateboarding on a park\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "inputs = tokenizer(query_text, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Construct the query for OpenSearch\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"caption_vec\": {\n",
    "                \"vector\": query_embedding.tolist(),\n",
    "                \"k\": 5  # Number of nearest neighbors to retrieve\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = client.search(index=index_name, body=query)\n",
    "print(\"KNN Search Results:\")\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Caption: {hit['_source']['caption']}, Score: {hit['_score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPDW",
   "language": "python",
   "name": "1env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
